{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2d21fe81",
   "metadata": {},
   "source": [
    "# Handwritten Digits Recognizer\n",
    "\n",
    "### Steps\n",
    "\n",
    "- Load MNIST dataset (handwritten digits) and visualize sample digits\n",
    "- Write basic preprocessing of dataset: normalize images, split train/test sets.\n",
    "- Implement first simple feedforward NN\n",
    "- Train subset (~1-5k images) quick results\n",
    "- Evaluate accuracy -> even if low\n",
    "- Upgrade NN to simple CN (1-2 conv layers)\n",
    "- Train CNN on MNIST -> aim for 95% accuracy\n",
    "- Log results, plot loss/accuracy curves.\n",
    "- Finalize CNN -> add droput/batch norm\n",
    "- Train an ML model (SVM - to compare performance with NN and CNN) \n",
    "- save trained model and write simple inference script.\n",
    "- Optional: create basic CLI to input image and get predicted digits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d342c426",
   "metadata": {},
   "source": [
    "# Dataset\n",
    "\n",
    "Content\n",
    "\n",
    "The MNIST database of handwritten digits has a training set of 60,000 examples, and a test set of 10,000 examples. .\n",
    "\n",
    "Four files are available:\n",
    "\n",
    "- train-images-idx3-ubyte.gz: training set images (9912422 bytes)\n",
    "- train-labels-idx1-ubyte.gz: training set labels (28881 bytes)\n",
    "- t10k-images-idx3-ubyte.gz: test set images (1648877 bytes)\n",
    "- t10k-labels-idx1-ubyte.gz: test set labels (4542 bytes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8310a12",
   "metadata": {},
   "source": [
    "Goal: To create a CNN that can detect the handwritten digits. In the end I should have a CLI app that I can input an image and output the correct digit."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9741b0be",
   "metadata": {},
   "source": [
    "# First Tasks\n",
    "\n",
    "- Create reader and plotter for minst dataset\n",
    "- Visualize the images. \n",
    "- Preprocess dataset (Clean, Normalize, etc.)\n",
    "- Get statistical metrics of dataset and Plot metrics."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84ba33cc",
   "metadata": {},
   "source": [
    "IDX format\n",
    "\n",
    "- Magic Number\n",
    "- size in dimension 0\n",
    "- size in dimension 1\n",
    "- size in dimension 2\n",
    "- ...\n",
    "- size in dimension N\n",
    "- data\n",
    "\n",
    "magic number is 4 bytes integers with two bytes set as 0 and the other two bytes used to describe:\n",
    "- basic data type used (3rd byte)\n",
    "- number of dimensions of the stored array (4th byte)\n",
    "\n",
    "data types:\n",
    "- 0x08: unsigned byte \n",
    "- 0x09: signed byte \n",
    "- 0x0B: short (2 bytes) \n",
    "- 0x0C: int (4 bytes) \n",
    "- 0x0D: float (4 bytes) \n",
    "- 0x0E: double (8 bytes)\n",
    "\n",
    "4th byte codes the number of dimensions of the vector/matrix:\n",
    "1 would be for vectors, 2 for two dimension matrices, etc.\n",
    "\n",
    "Then size of each dimension (4-byte integers, MSB first, big-endian)\n",
    "\n",
    "data itself is stored in a C array (row-major first) where the index in the last dimension change the fastest\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "68edeefc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "\n",
    "mnst_dataset = {\n",
    "    'train_images': 'MNIST_dataset/train-images-idx3-ubyte/train-images-idx3-ubyte',\n",
    "    'train_labels': 'MNIST_dataset/train-labels-idx1-ubyte/train-labels-idx1-ubyte',\n",
    "    'test_images': 'MNIST_dataset/t10k-images-idx3-ubyte/t10k-images-idx3-ubyte',\n",
    "    'test_labels': 'MNIST_dataset/t10k-images-idx1-ubyte/t10k-images-idx1-ubyte',\n",
    "}\n",
    "\n",
    "train_images_filepath = os.path.join(os.getcwd(),mnst_dataset.get('train_images'))\n",
    "\n",
    "with open(train_images_filepath,'rb') as file:\n",
    "    magic_number = file.read(4) #0 0 (data type) (num of dimensions fo stored arrays)\n",
    "    dimension_1 = int.from_bytes(file.read(4), byteorder='big', signed=False) # 60000\n",
    "    dimension_2 = int.from_bytes(file.read(4), byteorder='big', signed=False)# 28\n",
    "    dimension_3 = int.from_bytes(file.read(4), byteorder='big', signed=False)# 28\n",
    "    data_np = np.frombuffer(file.read(), dtype=np.uint8) #grayscale (8-bit unsigned integer)\n",
    "data_np = data_np.reshape(dimension_1,dimension_2,dimension_3) #img pixels (img, row, column)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5e9455f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x312125790>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAGFCAYAAAASI+9IAAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjcsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvTLEjVAAAAAlwSFlzAAAPYQAAD2EBqD+naQAACTpJREFUeJzt3M+LTn0Dx/FzkIlkFjbUZMdyDKLZ0b1EUbPQJLNVUpIs1JCd0lAoUlJEkSyMJJuRjZWGP8BKovxIfqQozpPF/fHU/Tw91/c85nIZr9dqFvPpXI7h7XvX/a2bpmkqAKiqao63AMDfRAGAEAUAQhQACFEAIEQBgBAFAEIUAIh5VYfquu70WwHoQZ38v8pOCgCEKAAQogBAiAIAIQoAhCgAEKIAQIgCACEKAIQoABCiAECIAgAhCgCEKAAQogBAiAIAIQoAhCgAEKIAQIgCACEKAIQoABCiAECIAgAhCgCEKAAQogBAiAIAIQoAhCgAEKIAQIgCAKIAwD85KQAQogBAiAIAIQoAhCgAEKIAQIgCACEKAIQoABCiAECIAgAhCgCEKAAQogBAiAIAIQoAhCgAEKIAQIgCADHvx5dAqbVr1xZv9uzZ0+pFj42NFW8uXbpUvDl9+nTxZnp6unhDb3JSACBEAYAQBQBCFAAIUQAgRAGAEAUAQhQACFEAIEQBgBAFAEIUAIi6aZqm6kBd1518G/y2hoaGijdTU1PFm8WLF1e97N27d8WbJUuWzMhn4efq5K97JwUAQhQACFEAIEQBgBAFAEIUAAhRACBEAYAQBQBCFAAIUQAgRAGAmPfjS5g91q9fX7y5ceNG8aa/v7940+EdlP/w4cOH4s2XL1+6crnd8PBw8WZ6erpqo82vic45KQAQogBAiAIAIQoAhCgAEKIAQIgCACEKAIQoABCiAECIAgAhCgBE3XR4O1dd1518G/xXCxcubPV21qxZU7y5fPly8WZgYKB40+bPRdsL8dpcIHfs2LHizdWrV7vyHsbHx6s2jh492mpH1dHPnpMCACEKAIQoABCiAECIAgAhCgCEKAAQogBAiAIAIQoAhCgAEKIAQIgCADHvx5cws86dO9dqNzo6+tM/y++ozW2xixYtKt7cv3+/eLNx48bizeDgYPGGmeekAECIAgAhCgCEKAAQogBAiAIAIQoAhCgAEKIAQIgCACEKAIQoABAuxKOVtWvXFm82b97c6ll1XVfd0OYiuFu3bhVvJiYmqjaeP39evHn06FHx5u3bt8Wbv/76q2d/XynjpABAiAIAIQoAhCgAEKIAQIgCACEKAIQoABCiAECIAgAhCgCEKAAQddM0TdUBl1fNXkNDQ8Wbqamp4s3ixYurbrlz507xZnR0tHizYcOG4s3g4GDVxvnz54s3r169qrrh69evxZtPnz61elabdz49Pd3qWbNNJ3/dOykAEKIAQIgCACEKAIQoABCiAECIAgAhCgCEKAAQogBAiAIAIQoAxLwfXzIbrFy5snhz4MCB4k1/f3/x5vXr11UbL168KN5cvHixePPx48fize3bt7uymY0WLFjQard///7izY4dO1o960/kpABAiAIAIQoAhCgAEKIAQIgCACEKAIQoABCiAECIAgAhCgCEKAAQogBAuCW1R/X19bXaTUxMFG82bdpUvPnw4UPxZmxsrGrj4cOHXbuBk963fPnyX/0RZjUnBQBCFAAIUQAgRAGAEAUAQhQACFEAIEQBgBAFAEIUAAhRACBEAYBwIV6PWr16datdm8vt2ti6dWvx5v79+zPyWYCfx0kBgBAFAEIUAAhRACBEAYAQBQBCFAAIUQAgRAGAEAUAQhQACFEAIFyI16NOnDjRalfXdVcuqnO5Hf9uzpzyf19++/bNS+xBTgoAhCgAEKIAQIgCACEKAIQoABCiAECIAgAhCgCEKAAQogBAiAIA4UK8LtiyZUvxZmhoqNWzmqYp3kxOTrZ6Fvw/l9u1+Vn97vHjx178DHJSACBEAYAQBQBCFAAIUQAgRAGAEAUAQhQACFEAIEQBgBAFAEIUAAgX4nXBggULijfz589v9ayXL18Wb65du9bqWfS+vr6+4s2RI0eqbpiammq1O3jw4E//LPzgpABAiAIAIQoAhCgAEKIAQIgCACEKAIQoABCiAECIAgAhCgCEKAAQogBAuCV1lvn8+XPx5sWLFzPyWfj1N56Oj48Xbw4cOFC8efbsWfHm+PHjVRsfP35staMzTgoAhCgAEKIAQIgCACEKAIQoABCiAECIAgAhCgCEKAAQogBAiAIA4UK8WWZycvJXfwT+h6GhoVbvqM1Fddu3by/e3Lx5s3gzMjJSvKE3OSkAEKIAQIgCACEKAIQoABCiAECIAgAhCgCEKAAQogBAiAIAIQoAhAvxuqCu665svtu2bVvxZu/eva2eRVXt27ev+DUcOnSo1avr7+8v3ly5cqV4MzY2Vrxh9nBSACBEAYAQBQBCFAAIUQAgRAGAEAUAQhQACFEAIEQBgBAFAEIUAAgX4nVB0zRd2Xy3dOnS4s2pU6eKNxcuXCjevHnzpnjz3fDwcPFm586dxZtVq1YVbwYGBoo3T58+rdq4e/du8ebMmTOtnsWfy0kBgBAFAEIUAAhRACBEAYAQBQBCFAAIUQAgRAGAEAUAQhQACFEAIFyIN8vMnTu3eLN79+7izcjISPHm/fv3VRsrVqyoetWDBw+KN/fu3Wv1rMOHD7faQQknBQBCFAAIUQAgRAGAEAUAQhQACFEAIEQBgBAFAEIUAAhRACBEAYAQBQCibpqmqTpQ13Un38Z/MDAwUPxerl+/3updrlu3riu/B21+Hjr8Ufsp3rx5U7y5evVq8Wbv3r3FG/hVOvkz6KQAQIgCACEKAIQoABCiAECIAgAhCgCEKAAQogBAiAIAIQoAhCgAEC7E61HLli1rtdu1a1fxZnx8vKcvxDt58mTx5uzZs8WbJ0+eFG/gd+JCPACK+M9HAIQoABCiAECIAgAhCgCEKAAQogBAiAIAIQoAhCgAEKIAQLgQD+AP0XRwKaWTAgAhCgCEKAAQogBAiAIAIQoAhCgAEKIAQIgCACEKAIQoABCiAECIAgAhCgCEKAAQogBAiAIAIQoAhCgAEKIAQIgCACEKAIQoABCiAECIAgAhCgCEKAAQogBAiAIAIQoAhCgAEKIAQIgCACEKAIQoABCiAECIAgAhCgCEKAAQogBAiAIAIQoAhCgAEKIAQIgCACEKAIQoABCiAECIAgAhCgCEKAAQogBAzKs61DRNp98KwG/KSQGAEAUAQhQACFEAIEQBgBAFAEIUAAhRACBEAYDqb/8CIjEv4AjgWVMAAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.axis('off')\n",
    "plt.imshow(data_np[1],cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3c1b73ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "from read_mnist import (\n",
    "    read_imgages_idx,\n",
    "    read_labels_idx,\n",
    "    show_image,\n",
    "    standardize_data,\n",
    "    split,\n",
    "    convert_numpy_to_flatten_tensor,\n",
    ")\n",
    "from mnist_neural_net import ImageNeuralNet, ModelTraining\n",
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d8d20938",
   "metadata": {},
   "outputs": [],
   "source": [
    "mnst_dataset = {\n",
    "    'train_images': 'MNIST_dataset/train-images-idx3-ubyte/train-images-idx3-ubyte',\n",
    "    'train_labels': 'MNIST_dataset/train-labels-idx1-ubyte/train-labels-idx1-ubyte',\n",
    "    'test_images': 'MNIST_dataset/t10k-images-idx3-ubyte/t10k-images-idx3-ubyte',\n",
    "    'test_labels': 'MNIST_dataset/t10k-labels-idx1-ubyte/t10k-labels-idx1-ubyte',\n",
    "}\n",
    "\n",
    "train_images_filepath = os.path.join(os.getcwd(),mnst_dataset.get('train_images'))\n",
    "train_labels_filepath = os.path.join(os.getcwd(),mnst_dataset.get('train_labels'))\n",
    "test_images_filepath = os.path.join(os.getcwd(),mnst_dataset.get('test_images'))\n",
    "test_labels_filepath = os.path.join(os.getcwd(),mnst_dataset.get('test_labels'))\n",
    "\n",
    "train_images = read_imgages_idx(train_images_filepath)\n",
    "train_labels = read_labels_idx(train_labels_filepath)\n",
    "test_images = read_imgages_idx(test_images_filepath)\n",
    "test_labels = read_labels_idx(test_labels_filepath)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a101340a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Standardized with mean: 33.35 and std: 78.6\n",
      "Standardized with mean: 33.18 and std: 78.43\n"
     ]
    }
   ],
   "source": [
    "X_train, X_val, y_train, y_val = split(train_images, train_labels, 0.2)\n",
    "\n",
    "X_train= standardize_data(X_train)\n",
    "X_val = standardize_data(X_val)\n",
    "\n",
    "X_train = convert_numpy_to_flatten_tensor(X_train)\n",
    "X_val = convert_numpy_to_flatten_tensor(X_val)\n",
    "\n",
    "y_train = torch.tensor(y_train, dtype=torch.float32)\n",
    "y_val = torch.tensor(y_val, dtype=torch.float32)\n",
    "\n",
    "# Need to modify y values (targets) to one-hot encoded \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ef97f66d",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_numbers = [0,1,2,3,4,5,6,7,8,9]\n",
    "image_identity = torch.eye(len(image_numbers))\n",
    "\n",
    "temp_matrix = {}\n",
    "for i,image_num in enumerate(image_numbers):\n",
    "    temp_matrix[image_num] = image_identity[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8a71f385",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_one_hot= torch.zeros((y_train.shape[0],10))\n",
    "y_val_one_hot = torch.zeros((y_val.shape[0],10))\n",
    "for i, y_value in enumerate(y_train):\n",
    "    y_train_one_hot[i] = temp_matrix.get(y_value.item())\n",
    "for i, y_value in enumerate(y_val):\n",
    "    y_val_one_hot[i] = temp_matrix.get(y_value.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b3c94ad3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageNeuralNet(nn.Module):\n",
    "    def __init__(self,image_pixels): # image_pixel = 28*28 => 784\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(image_pixels,512)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(512,128)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.fc3 = nn.Linear(128,64)\n",
    "        self.relu3 = nn.ReLU()\n",
    "        self.output_layer = nn.Linear(64,10) \n",
    "\n",
    "    def forward(self, image):\n",
    "        x = self.relu1(self.fc1(image))\n",
    "        x = self.relu2(self.fc2(x))\n",
    "        x = self.relu3(self.fc3(x))\n",
    "        x = self.output_layer(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f702f296",
   "metadata": {},
   "outputs": [],
   "source": [
    "mps_device = torch.device(\"mps\")\n",
    "image_pixels = 28*28\n",
    "model = ImageNeuralNet(image_pixels).to(mps_device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "\n",
    "#feedforward_model = ModelTraining(model,optimizer,loss_function)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91fc100c",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_set = (X_train,y_train_one_hot)\n",
    "val_set = (X_val, y_val_one_hot) \n",
    "\n",
    "epochs = 5\n",
    "batch_size = 32\n",
    "#feedforward_model.train_loop(training_set, val_set, epochs, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f6c2cd35",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12000"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(val_set[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "973c03cd",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Expected input batch_size (32) to match target batch_size (12000).",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[21]\u001b[39m\u001b[32m, line 52\u001b[39m\n\u001b[32m     49\u001b[39m         y_val = v_labels[start:end].to(torch.device(\u001b[33m\"\u001b[39m\u001b[33mmps\u001b[39m\u001b[33m\"\u001b[39m))\n\u001b[32m     51\u001b[39m         y_val_pred = model(X_val)\n\u001b[32m---> \u001b[39m\u001b[32m52\u001b[39m         loss = \u001b[43mloss_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_val_pred\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_val_one_hot\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     53\u001b[39m         v_total_loss += loss\n\u001b[32m     56\u001b[39m average_train_loss = total_loss / dataset_size\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/handwritten-digit-recognizer/digits/lib/python3.12/site-packages/torch/nn/modules/module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/handwritten-digit-recognizer/digits/lib/python3.12/site-packages/torch/nn/modules/module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/handwritten-digit-recognizer/digits/lib/python3.12/site-packages/torch/nn/modules/loss.py:1385\u001b[39m, in \u001b[36mCrossEntropyLoss.forward\u001b[39m\u001b[34m(self, input, target)\u001b[39m\n\u001b[32m   1383\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor, target: Tensor) -> Tensor:\n\u001b[32m   1384\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Runs the forward pass.\"\"\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1385\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcross_entropy\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1386\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   1387\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1388\u001b[39m \u001b[43m        \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1389\u001b[39m \u001b[43m        \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mignore_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1390\u001b[39m \u001b[43m        \u001b[49m\u001b[43mreduction\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mreduction\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1391\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlabel_smoothing\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mlabel_smoothing\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1392\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/handwritten-digit-recognizer/digits/lib/python3.12/site-packages/torch/nn/functional.py:3458\u001b[39m, in \u001b[36mcross_entropy\u001b[39m\u001b[34m(input, target, weight, size_average, ignore_index, reduce, reduction, label_smoothing)\u001b[39m\n\u001b[32m   3456\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m size_average \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m reduce \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   3457\u001b[39m     reduction = _Reduction.legacy_get_string(size_average, reduce)\n\u001b[32m-> \u001b[39m\u001b[32m3458\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_C\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_nn\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcross_entropy_loss\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   3459\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   3460\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3461\u001b[39m \u001b[43m    \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3462\u001b[39m \u001b[43m    \u001b[49m\u001b[43m_Reduction\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_enum\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreduction\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3463\u001b[39m \u001b[43m    \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3464\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlabel_smoothing\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3465\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mValueError\u001b[39m: Expected input batch_size (32) to match target batch_size (12000)."
     ]
    }
   ],
   "source": [
    "train_list = []\n",
    "validation_list = []\n",
    "best_val_loss = 10000\n",
    "best_model = None\n",
    "best_metrics = None\n",
    "for epoch in range(epochs): # start with 50 epochs\n",
    "\n",
    "    # Training\n",
    "    total_loss = 0\n",
    "    dataset_size = len(training_set[0])\n",
    "    images, labels = training_set\n",
    "    model.train()\n",
    "    for i in range(0, dataset_size, batch_size):\n",
    "        if (i == dataset_size - 1) and (dataset_size % batch_size != 0):\n",
    "            batch_size = dataset_size % batch_size\n",
    "        start = i\n",
    "        end = i + batch_size\n",
    "\n",
    "        X_train = images[start:end].to(torch.device(\"mps\"))\n",
    "        y_train = labels[start:end].to(torch.device(\"mps\"))\n",
    "        \n",
    "        # Forward pass.\n",
    "        y_pred = model(X_train)\n",
    "        loss = loss_function(y_pred, y_train) #Calculates Loss (y_pred - y_true)\n",
    "        # Backward pass and optimization\n",
    "        optimizer.zero_grad() # Reset the gradients of all optimized\n",
    "        loss.backward() # Computes the gradeint of current tensor wrt graph leaves\n",
    "                        # Traverses teh computational graph (built during forward pass)\n",
    "                        # It calculates the gradients of the loss with respect to all tensors\n",
    "                        # in the graph.\n",
    "        optimizer.step() # Uses the gradients to updates the parameters, minimizing loss\n",
    "                                # and improving model's performance.\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    # Evaluation\n",
    "    model.eval()\n",
    "    val_set_size = len(val_set[0])\n",
    "    v_images, v_labels = val_set\n",
    "    v_total_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for i in range(0, val_set_size, batch_size):\n",
    "            if (i == val_set_size - 1) and (val_set_size % batch_size != 0):\n",
    "                print('HERE')\n",
    "                batch_size = val_set_size % batch_size\n",
    "            start = i\n",
    "            end = i + batch_size\n",
    "            X_val = v_images[start:end].to(torch.device(\"mps\"))\n",
    "            y_val = v_labels[start:end].to(torch.device(\"mps\"))\n",
    "            \n",
    "            y_val_pred = model(X_val)\n",
    "            loss = loss_function(y_val_pred, y_val)\n",
    "            v_total_loss += loss\n",
    "    \n",
    "\n",
    "    average_train_loss = total_loss / dataset_size\n",
    "    average_val_loss = v_total_loss/ val_set_size\n",
    "\n",
    "    train_list.append(average_train_loss)\n",
    "    validation_list.append(average_val_loss)\n",
    "\n",
    "    if average_val_loss <= best_val_loss:\n",
    "        best_val_loss = average_train_loss\n",
    "        best_model = model\n",
    "        #self.best_metrics = (epoch, y_val, y_val_pred)\n",
    "\n",
    "    print(f\"Epoch {epoch + 1}\")\n",
    "    print(f\"Train Loss: {train_list[-1]}\")\n",
    "    print(f\"Val Loss: {validation_list[-1]}\")\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30a9fc4a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "digits",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
