{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2d21fe81",
   "metadata": {},
   "source": [
    "# Handwritten Digits Recognizer\n",
    "\n",
    "### Steps\n",
    "\n",
    "- Load MNIST dataset (handwritten digits) and visualize sample digits\n",
    "- Write basic preprocessing of dataset: normalize images, split train/test sets.\n",
    "- Implement first simple feedforward NN\n",
    "- Train subset (~1-5k images) quick results\n",
    "- Evaluate accuracy -> even if low\n",
    "- Upgrade NN to simple CN (1-2 conv layers)\n",
    "- Train CNN on MNIST -> aim for 95% accuracy\n",
    "- Log results, plot loss/accuracy curves.\n",
    "- Finalize CNN -> add droput/batch norm\n",
    "- Train an ML model (SVM - to compare performance with NN and CNN) \n",
    "- save trained model and write simple inference script.\n",
    "- Optional: create basic CLI to input image and get predicted digits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d342c426",
   "metadata": {},
   "source": [
    "# Dataset\n",
    "\n",
    "Content\n",
    "\n",
    "The MNIST database of handwritten digits has a training set of 60,000 examples, and a test set of 10,000 examples. .\n",
    "\n",
    "Four files are available:\n",
    "\n",
    "- train-images-idx3-ubyte.gz: training set images (9912422 bytes)\n",
    "- train-labels-idx1-ubyte.gz: training set labels (28881 bytes)\n",
    "- t10k-images-idx3-ubyte.gz: test set images (1648877 bytes)\n",
    "- t10k-labels-idx1-ubyte.gz: test set labels (4542 bytes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8310a12",
   "metadata": {},
   "source": [
    "Goal: To create a CNN that can detect the handwritten digits. In the end I should have a CLI app that I can input an image and output the correct digit."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9741b0be",
   "metadata": {},
   "source": [
    "# First Tasks\n",
    "\n",
    "- Create reader and plotter for minst dataset\n",
    "- Visualize the images. \n",
    "- Preprocess dataset (Clean, Normalize, etc.)\n",
    "- Get statistical metrics of dataset and Plot metrics."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84ba33cc",
   "metadata": {},
   "source": [
    "IDX format\n",
    "\n",
    "- Magic Number\n",
    "- size in dimension 0\n",
    "- size in dimension 1\n",
    "- size in dimension 2\n",
    "- ...\n",
    "- size in dimension N\n",
    "- data\n",
    "\n",
    "magic number is 4 bytes integers with two bytes set as 0 and the other two bytes used to describe:\n",
    "- basic data type used (3rd byte)\n",
    "- number of dimensions of the stored array (4th byte)\n",
    "\n",
    "data types:\n",
    "- 0x08: unsigned byte \n",
    "- 0x09: signed byte \n",
    "- 0x0B: short (2 bytes) \n",
    "- 0x0C: int (4 bytes) \n",
    "- 0x0D: float (4 bytes) \n",
    "- 0x0E: double (8 bytes)\n",
    "\n",
    "4th byte codes the number of dimensions of the vector/matrix:\n",
    "1 would be for vectors, 2 for two dimension matrices, etc.\n",
    "\n",
    "Then size of each dimension (4-byte integers, MSB first, big-endian)\n",
    "\n",
    "data itself is stored in a C array (row-major first) where the index in the last dimension change the fastest\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "68edeefc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "\n",
    "mnst_dataset = {\n",
    "    'train_images': 'MNIST_dataset/train-images-idx3-ubyte/train-images-idx3-ubyte',\n",
    "    'train_labels': 'MNIST_dataset/train-labels-idx1-ubyte/train-labels-idx1-ubyte',\n",
    "    'test_images': 'MNIST_dataset/t10k-images-idx3-ubyte/t10k-images-idx3-ubyte',\n",
    "    'test_labels': 'MNIST_dataset/t10k-images-idx1-ubyte/t10k-images-idx1-ubyte',\n",
    "}\n",
    "\n",
    "train_images_filepath = os.path.join(os.getcwd(),mnst_dataset.get('train_images'))\n",
    "\n",
    "with open(train_images_filepath,'rb') as file:\n",
    "    magic_number = file.read(4) #0 0 (data type) (num of dimensions fo stored arrays)\n",
    "    dimension_1 = int.from_bytes(file.read(4), byteorder='big', signed=False) # 60000\n",
    "    dimension_2 = int.from_bytes(file.read(4), byteorder='big', signed=False)# 28\n",
    "    dimension_3 = int.from_bytes(file.read(4), byteorder='big', signed=False)# 28\n",
    "    data_np = np.frombuffer(file.read(), dtype=np.uint8) #grayscale (8-bit unsigned integer)\n",
    "data_np = data_np.reshape(dimension_1,dimension_2,dimension_3) #img pixels (img, row, column)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "5e9455f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x1162feed0>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.axis('off')\n",
    "plt.imshow(data_np[1],cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "3c1b73ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "from read_mnist import (\n",
    "    read_imgages_idx,\n",
    "    read_labels_idx,\n",
    "    show_image,\n",
    "    standardize_data,\n",
    "    split,\n",
    ")\n",
    "from mnist_neural_net import ImageClassifier, ModelTraining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "52084777",
   "metadata": {},
   "outputs": [],
   "source": [
    "def testsplit(images_data, labels, val_ratio):\n",
    "\n",
    "    number_of_images = len(labels)\n",
    "    shuffled_sequence = np.random.permutation(number_of_images)\n",
    "\n",
    "    shuffled_images = images_data[shuffled_sequence]\n",
    "    shuffled_labels = labels[shuffled_sequence]\n",
    "    \n",
    "    split_index = int((1 - val_ratio) * number_of_images)\n",
    "    print(split_index)\n",
    "    X_train = shuffled_images[:split_index]\n",
    "    X_val = shuffled_images[split_index:]\n",
    "    y_train = shuffled_labels[:split_index]\n",
    "    y_val = shuffled_labels[split_index:]\n",
    "\n",
    "    return X_train, X_val, y_train, y_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8d20938",
   "metadata": {},
   "outputs": [],
   "source": [
    "mnst_dataset = {\n",
    "    'train_images': 'MNIST_dataset/train-images-idx3-ubyte/train-images-idx3-ubyte',\n",
    "    'train_labels': 'MNIST_dataset/train-labels-idx1-ubyte/train-labels-idx1-ubyte',\n",
    "    'test_images': 'MNIST_dataset/t10k-images-idx3-ubyte/t10k-images-idx3-ubyte',\n",
    "    'test_labels': 'MNIST_dataset/t10k-labels-idx1-ubyte/t10k-labels-idx1-ubyte',\n",
    "}\n",
    "\n",
    "train_images_filepath = os.path.join(os.getcwd(),mnst_dataset.get('train_images'))\n",
    "train_labels_filepath = os.path.join(os.getcwd(),mnst_dataset.get('train_labels'))\n",
    "test_images_filepath = os.path.join(os.getcwd(),mnst_dataset.get('test_images'))\n",
    "test_labels_filepath = os.path.join(os.getcwd(),mnst_dataset.get('test_labels'))\n",
    "\n",
    "train_images = read_imgages_idx(train_images_filepath)\n",
    "train_labels = read_labels_idx(train_labels_filepath)\n",
    "test_images = read_imgages_idx(test_images_filepath)\n",
    "test_labels = read_labels_idx(test_labels_filepath)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "a101340a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "48000\n"
     ]
    }
   ],
   "source": [
    "X_train, X_val, y_train, y_val = testsplit(train_images, train_labels, 0.2)\n",
    "\n",
    "X_train_standard = standardize_data(X_train)\n",
    "X_val_standard = standardize_data(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "5aaf8d63",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import matplotlib.pylab as plt\n",
    "from datetime import datetime\n",
    "\n",
    "class ImageNeuralNet(nn.Module):\n",
    "    def __init__(self,image_pixels): # image_pixel = 28*28 => 784\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(image_pixels,512)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(512,128)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.fc3 = nn.Linear(128,64)\n",
    "        self.relu3 = nn.ReLU()\n",
    "        self.output_layer = nn.Linear(64,1)\n",
    "\n",
    "    def forward(self, image):\n",
    "        x = self.relu1(self.fc1(image))\n",
    "        x = self.relu2(self.fc2(x))\n",
    "        x = self.relu3(self.fc3(x))\n",
    "        x = self.output_layer(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "f702f296",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "mps_device = torch.device(\"mps\")\n",
    "image_pixels = 28*28\n",
    "model = ImageNeuralNet(image_pixels).to(mps_device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "loss_function = nn.BCEWithLogitsLoss()\n",
    "\n",
    "feedforward_model = ModelTraining(model,optimizer,loss_function)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91fc100c",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[165]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mfeedforward_model\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain_loop\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train_standard\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_val_standard\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m32\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/handwritten-digit-recognizer/mnist_neural_net.py:43\u001b[39m, in \u001b[36mtrain_loop\u001b[39m\u001b[34m(self, train_set, val_set, number_of_epochs, batch_size)\u001b[39m\n\u001b[32m     38\u001b[39m     self.best_metrics = None\n\u001b[32m     41\u001b[39m def train_loop(self, train_set, val_set, number_of_epochs, batch_size):\n\u001b[32m     42\u001b[39m     for epoch in range(number_of_epochs): # start with 50 epochs\n\u001b[32m---> \u001b[39m\u001b[32m43\u001b[39m \n\u001b[32m     44\u001b[39m         # Training\n\u001b[32m     45\u001b[39m         total_loss = 0\n\u001b[32m     46\u001b[39m         dataset_size = len(train_set)\n",
      "\u001b[31mValueError\u001b[39m: too many values to unpack (expected 2)"
     ]
    }
   ],
   "source": [
    "\n",
    "training_set = (X_train_standard,y_train)\n",
    "validation_set = (X_val, y_val)\n",
    "feedforward_model.train_loop(training_set, validation_set, 1, 32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4641e529",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "digits",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
